{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook with Example Config for Different Models / Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gelu-2L\n",
    "\n",
    "An example of a toy language model we're able to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 4096-L1-0.004-LR-0.0012-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 4.194304\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 2097.152\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 48 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5d33022d3a4e8790cc701b0b912b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is tokenized! Updating config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjbloom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/mats_sae_training/scripts/wandb/run-20240224_221048-z576nqkn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/z576nqkn' target=\"_blank\">4096-L1-0.004-LR-0.0012-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/z576nqkn' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/z576nqkn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 13.375\n",
      "New distances: 10.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48828| MSE Loss 0.155 | L1 0.109: 100%|█████████▉| 199999488/200000000 [51:38<00:00, 73624.28it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/l4oixhh5/final_sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_4096.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2c973f64d94e4e91a6761a7c544ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.087 MB of 8.087 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>▁▃▆▇▇███████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇</td></tr><tr><td>losses/mse_loss</td><td>█▆▄▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▆▆▆▆▇▇▇▇▇▇▇█▇████████▇█████▇███▇███████</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▂▅▇▅▂▃▅▅▅▃▁▃▂█▆▂▅▇▃▇▃▇▃▆▇▅▃▅▃▅▅▁▆▂▂▅▅▅▂▆</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>█▄▃▄▃▃▃▃▂▂▂▂▂▃▃▂▂▂▂▂▂▃▂▃▂▂▂▁▂▁▂▂▃▂▁▂▂▃▂▂</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▄▅▃▅▅▅▄▇▂▁▃▄▄▄▆▄▄▅▅▃▅█▂▇▄▄▃▂▂▁▃▃▅▄▂▄▄▇▅▅</td></tr><tr><td>metrics/explained_variance</td><td>▁▃▅▆▆▆▇▇▇▇▇▇▇█▇█████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▆▄▃▃▃▃▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>█▁▃▃▃▄▃▄▄▄▅▆▅▅▆▅▅▄▅▄▅▄▄▄▄▄▅▄▅▃▃▄▄▃▅▄▄▄▃▂</td></tr><tr><td>metrics/l2_norm</td><td>▁▆▆▇▇█▇▇▇▇██████████████▇█████▇█▇███████</td></tr><tr><td>metrics/l2_ratio</td><td>▁▆▆▇▇█▇▇▇▇████████████▇█▇█████▇█▇███████</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▂▂▃▄▄▅▅▆▆▇▇▇▇▇████████████████████████</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▁▂▂▂▃▃▄▄▅▅▆▆▆▆▇▇▇████▇██▇▇███▇███████</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▂▂▂▃▃▄▅▅▅▆▆▆▆▇▇█▇▆▆▇▇▇█▇▇█▇█▇▇█▇██</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▆▇▇▇▇▇██▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0012</td></tr><tr><td>details/n_training_tokens</td><td>199966720</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>27.46582</td></tr><tr><td>losses/mse_loss</td><td>0.15689</td></tr><tr><td>losses/overall_loss</td><td>0.26676</td></tr><tr><td>metrics/CE_loss_score</td><td>0.85547</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>10.375</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>4.71875</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>3.76562</td></tr><tr><td>metrics/explained_variance</td><td>0.74219</td></tr><tr><td>metrics/explained_variance_std</td><td>0.10742</td></tr><tr><td>metrics/l0</td><td>26.74658</td></tr><tr><td>metrics/l2_norm</td><td>19.625</td></tr><tr><td>metrics/l2_ratio</td><td>0.87109</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-3.95979</td></tr><tr><td>sparsity/below_1e-5</td><td>1117</td></tr><tr><td>sparsity/below_1e-6</td><td>379</td></tr><tr><td>sparsity/dead_features</td><td>131</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>83.35889</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">4096-L1-0.004-LR-0.0012-Tokens-1.000e+08</strong> at: <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/z576nqkn' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/z576nqkn</a><br/>Synced 7 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240224_221048-z576nqkn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 4096-L1-0.004-LR-0.0012-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 4.194304\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 2097.152\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 48 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9564933112459ba706027c09d6bc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is tokenized! Updating config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48828| MSE Loss 0.155 | L1 0.109: : 200003584it [51:51, 73624.28it/s]                             /home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (z576nqkn) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/mats_sae_training/scripts/wandb/run-20240224_230257-p2vsoomd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/p2vsoomd' target=\"_blank\">4096-L1-0.004-LR-0.0012-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/p2vsoomd' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/p2vsoomd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 13.375\n",
      "New distances: 10.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (z576nqkn) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "48828| MSE Loss 0.155 | L1 0.109: : 200003584it [52:15, 63780.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/roopm8zg/final_sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_4096.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc8d272288e4d6795eb907d4342a4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.087 MB of 8.087 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>▁▄▆▇▇▇▇▇█▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>losses/mse_loss</td><td>█▆▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▆▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▅▅▆▆▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇██▇█▇███▇███▇███████</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▂▅▇▅▂▃▅▅▅▃▁▃▂█▆▂▅▇▃▇▃▇▃▆▇▅▃▅▃▅▅▁▆▂▂▅▅▅▂▆</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▃▃▂▂▃▂▂▂▃▁▃▂▂▁▁▁▁▂▁▂▂▁▁▁▂▂▂</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▄▅▃▅▅▅▄▇▂▁▃▄▄▄▆▄▄▅▅▃▅█▂▇▄▄▃▂▂▁▃▃▅▄▂▄▄▇▅▅</td></tr><tr><td>metrics/explained_variance</td><td>▁▃▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▆▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>▇▁▂▃▃▂▄▄▆▃▇▂▆█▃▅▄▄▃▄▅▅▆▅▃▆▄▆▅▃▄▄▅▆▅▄▆▆▅▄</td></tr><tr><td>metrics/l2_norm</td><td>▁▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆████████████████████</td></tr><tr><td>metrics/l2_ratio</td><td>▁▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆████████████████████</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▂▃▃▄▄▅▆▆▆▇▇▇▇▇████████████████████████</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▁▂▂▂▃▃▄▄▅▅▅▆▆▆▆▇▇█▇▇▇▇█▇▇▇▇▇▇▇▇▇█▇█▇▇</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▂▂▂▃▄▄▄▅▆▅▆▅▆▇▇▇▆▇▆▇▆▇▇▇▇▆▆██▇▇█▇▇</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇██▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0012</td></tr><tr><td>details/n_training_tokens</td><td>199966720</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>27.83203</td></tr><tr><td>losses/mse_loss</td><td>0.14315</td></tr><tr><td>losses/overall_loss</td><td>0.25448</td></tr><tr><td>metrics/CE_loss_score</td><td>0.86328</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>10.375</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>4.65625</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>3.76562</td></tr><tr><td>metrics/explained_variance</td><td>0.76562</td></tr><tr><td>metrics/explained_variance_std</td><td>0.1001</td></tr><tr><td>metrics/l0</td><td>27.26099</td></tr><tr><td>metrics/l2_norm</td><td>21.0</td></tr><tr><td>metrics/l2_ratio</td><td>0.92578</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-3.94798</td></tr><tr><td>sparsity/below_1e-5</td><td>1125</td></tr><tr><td>sparsity/below_1e-6</td><td>383</td></tr><tr><td>sparsity/dead_features</td><td>138</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>83.99756</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">4096-L1-0.004-LR-0.0012-Tokens-1.000e+08</strong> at: <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/p2vsoomd' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/p2vsoomd</a><br/>Synced 7 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240224_230257-p2vsoomd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 4096-L1-0.004-LR-0.0012-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 4.194304\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 2097.152\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 48 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae74a588a1e44299dbd4043a45a3076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is tokenized! Updating config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (p2vsoomd) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/mats_sae_training/scripts/wandb/run-20240224_235710-4esr5nkz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/4esr5nkz' target=\"_blank\">4096-L1-0.004-LR-0.0012-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/4esr5nkz' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/4esr5nkz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 13.375\n",
      "New distances: 10.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63| MSE Loss 0.799 | L1 1.297:   0%|          | 262144/200000000 [00:01<15:52, 209612.65it/s]/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (p2vsoomd) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "48828| MSE Loss 0.143 | L1 0.110: : 200003584it [54:13, 61469.24it/s]\n",
      "48828| MSE Loss 0.156 | L1 0.110: 100%|█████████▉| 199999488/200000000 [52:16<00:00, 67203.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/th9vzapx/final_sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_4096.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4ae38cfd054fdc9d0f1a98fe85677f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='8.087 MB of 8.087 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>▁▄▆▇▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>losses/mse_loss</td><td>█▆▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▅▆▆▆▇▇▇▇▇▇██▇████████▇█████▇███▇███████</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▂▅▇▅▂▃▅▅▅▃▁▃▂█▆▂▅▇▃▇▃▇▃▆▇▅▃▅▃▅▅▁▆▂▂▅▅▅▂▆</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>█▅▃▃▃▃▂▃▂▂▂▂▂▃▃▂▂▂▂▂▂▃▂▃▂▂▂▁▂▁▂▁▃▂▁▂▂▂▂▂</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▄▅▃▅▅▅▄▇▂▁▃▄▄▄▆▄▄▅▅▃▅█▂▇▄▄▃▂▂▁▃▃▅▄▂▄▄▇▅▅</td></tr><tr><td>metrics/explained_variance</td><td>▁▃▅▆▆▆▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▅▃▃▃▂▂▂▂▂▂▂▁▁▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▂▁</td></tr><tr><td>metrics/l0</td><td>█▂▄▅▆▅█▅▁▆▆▅▅▃▆▄▄▆▄▆▅▄▅▆▅▅▅▆▆▅▅▆▄▅▄▆▅▅▆▆</td></tr><tr><td>metrics/l2_norm</td><td>▁▆▆▇▇▇▇▇█▇██████████████████████████████</td></tr><tr><td>metrics/l2_ratio</td><td>▁▆▆▇▇▇▇▇█▇██████████████████████████████</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▂▃▃▄▄▅▆▆▆▇▇▇▇▇████████████████████████</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▁▁▂▂▃▃▄▄▅▅▅▆▆▆▇▇▇███▇▇█▇▇▇▇▇█▇█▇▇▇██▇</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▂▂▃▄▄▄▅▅▅▅▆▇▇██▇▇▇▇▇▇█▇▇▇▇▇▆▇▇▇█▇</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▇▇███▇▇████▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0012</td></tr><tr><td>details/n_training_tokens</td><td>199966720</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>27.58789</td></tr><tr><td>losses/mse_loss</td><td>0.15829</td></tr><tr><td>losses/overall_loss</td><td>0.26864</td></tr><tr><td>metrics/CE_loss_score</td><td>0.85547</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>10.375</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>4.71875</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>3.76562</td></tr><tr><td>metrics/explained_variance</td><td>0.73828</td></tr><tr><td>metrics/explained_variance_std</td><td>0.10938</td></tr><tr><td>metrics/l0</td><td>27.05713</td></tr><tr><td>metrics/l2_norm</td><td>19.75</td></tr><tr><td>metrics/l2_ratio</td><td>0.875</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-3.95843</td></tr><tr><td>sparsity/below_1e-5</td><td>1124</td></tr><tr><td>sparsity/below_1e-6</td><td>372</td></tr><tr><td>sparsity/dead_features</td><td>142</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>88.90942</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">4096-L1-0.004-LR-0.0012-Tokens-1.000e+08</strong> at: <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/4esr5nkz' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/4esr5nkz</a><br/>Synced 7 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240224_235710-4esr5nkz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 8192-L1-0.004-LR-0.0012-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 4.194304\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 2097.152\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 48 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef9bc2281154a6990b2688b1c84bd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is tokenized! Updating config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48828| MSE Loss 0.156 | L1 0.110: : 200003584it [52:29, 67203.60it/s]                             /home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (4esr5nkz) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/mats_sae_training/scripts/wandb/run-20240225_005000-y3kxuwlj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/y3kxuwlj' target=\"_blank\">8192-L1-0.004-LR-0.0012-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/y3kxuwlj' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/y3kxuwlj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 13.375\n",
      "New distances: 10.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (4esr5nkz) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "48828| MSE Loss 0.156 | L1 0.110: : 200003584it [52:48, 63130.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/h44gguob/final_sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_8192.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab45a0204a35477ca0dcadd02fdadcee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='16.118 MB of 16.118 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>▁██▅▇▃▆▄▆▆▄▇▅▄▃▃▁▅▅▅▂▂▁▃▄▃▄▄▂▁▂▂▂▃▃▃▂▃▁▃</td></tr><tr><td>losses/mse_loss</td><td>█▆▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▆▆▇▆▇▇▇▇▇▇▇█▇█▇██████▇█████▇███▇███████</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▂▅▇▅▂▃▅▅▅▃▁▃▂█▆▂▅▇▃▇▃▇▃▆▇▅▃▅▃▅▅▁▆▂▂▅▅▅▂▆</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>█▄▃▃▃▃▂▄▂▂▂▂▂▃▃▂▂▂▂▂▂▃▁▃▂▂▂▁▁▁▂▁▃▂▁▂▂▃▂▂</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▄▅▃▅▅▅▄▇▂▁▃▄▄▄▆▄▄▅▅▃▅█▂▇▄▄▃▂▂▁▃▃▅▄▂▄▄▇▅▅</td></tr><tr><td>metrics/explained_variance</td><td>▁▃▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▆▅▃▃▄▃▃▂▂▂▂▁▂▂▂▂▁▂▂▁▁▂▂▂▁▂▁▁▁▂▁▁▂▂▁▁▁▂▁</td></tr><tr><td>metrics/l0</td><td>█▅▃▂▃▁▂▂▂▂▁▂▁▂▁▁▁▂▂▂▁▁▁▂▂▁▂▂▁▁▁▁▁▂▂▂▁▁▁▁</td></tr><tr><td>metrics/l2_norm</td><td>▁▅▅▅▆▆▆▆▇▇███▇██▇█████▇█▇█████▇█▇███████</td></tr><tr><td>metrics/l2_ratio</td><td>▁▅▅▅▆▆▆▇▇▇███▇██▇▇████▇█▇▇████▇█▇███████</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▂▃▄▅▅▆▆▆▇▇▇▇▇▇████████████████████████</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▁▂▂▂▃▄▄▅▆▆▆▆▇▇▇██████▇█▇▇██████████▇█</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▂▂▃▃▃▄▅▅▅▆▆▆▆▇▇██████▇▇▇█▇▇█▇███▇▇█</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▅▆▇▇▇▇▇▇█▇▇▇█▇▇▇█████▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0012</td></tr><tr><td>details/n_training_tokens</td><td>199966720</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>24.53613</td></tr><tr><td>losses/mse_loss</td><td>0.154</td></tr><tr><td>losses/overall_loss</td><td>0.25215</td></tr><tr><td>metrics/CE_loss_score</td><td>0.87109</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>10.375</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>4.59375</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>3.76562</td></tr><tr><td>metrics/explained_variance</td><td>0.74609</td></tr><tr><td>metrics/explained_variance_std</td><td>0.10693</td></tr><tr><td>metrics/l0</td><td>24.4668</td></tr><tr><td>metrics/l2_norm</td><td>18.75</td></tr><tr><td>metrics/l2_ratio</td><td>0.82812</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-4.84764</td></tr><tr><td>sparsity/below_1e-5</td><td>4117</td></tr><tr><td>sparsity/below_1e-6</td><td>1848</td></tr><tr><td>sparsity/dead_features</td><td>778</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>234.87</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">8192-L1-0.004-LR-0.0012-Tokens-1.000e+08</strong> at: <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/y3kxuwlj' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/y3kxuwlj</a><br/>Synced 7 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240225_005000-y3kxuwlj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 8192-L1-0.004-LR-0.0012-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 4.194304\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 2097.152\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 48 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07dc54a494c04d9db8876bdb102dda95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is tokenized! Updating config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (y3kxuwlj) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/mats_sae_training/scripts/wandb/run-20240225_014425-yet79nwa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/yet79nwa' target=\"_blank\">8192-L1-0.004-LR-0.0012-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/yet79nwa' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/yet79nwa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 13.375\n",
      "New distances: 10.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115| MSE Loss 0.547 | L1 0.227:   0%|          | 471040/200000000 [00:04<1:04:27, 51593.19it/s]/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (y3kxuwlj) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "48828| MSE Loss 0.153 | L1 0.097: : 200003584it [54:30, 61154.30it/s]\n",
      "48828| MSE Loss 0.132 | L1 0.099: 100%|█████████▉| 199999488/200000000 [52:46<00:00, 74117.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/1v9hhytx/final_sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_8192.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcb1546206d4ce79cec11e77f59d442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='16.118 MB of 16.118 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>▆▇█▅▇▇▇▄▁▆▆▃▃▃▆▆▆▅▆▃▆▆▅▅▅▅▄▆▅▆▆▅▆▆▅▅▅▆▅▆</td></tr><tr><td>losses/mse_loss</td><td>█▆▅▅▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▆▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇██▇██▇█▇█▇█▇▇████</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▂▅▇▅▂▃▅▅▅▃▁▃▂█▆▂▅▇▃▇▃▇▃▆▇▅▃▅▃▅▅▁▆▂▂▅▅▅▂▆</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>█▅▃▄▃▃▃▄▂▂▂▂▂▃▃▂▂▃▂▂▂▃▂▃▂▂▂▁▁▁▂▂▃▂▁▂▂▃▂▂</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▄▅▃▅▅▅▄▇▂▁▃▄▄▄▆▄▄▅▅▃▅█▂▇▄▄▃▂▂▁▃▃▅▄▂▄▄▇▅▅</td></tr><tr><td>metrics/explained_variance</td><td>▁▃▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▆▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▄▃▁▂▁▁▁▁▂▁▂▁▂▁▂▁▁▁▂▁▂▁</td></tr><tr><td>metrics/l0</td><td>█▅▃▃▃▃▂▂▁▂▂▂▁▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>metrics/l2_norm</td><td>▁▃▃▃▃▄▃▄▃▄▄▄▄▄▄▄▄▄▄▄████████████████████</td></tr><tr><td>metrics/l2_ratio</td><td>▁▃▃▃▃▄▃▃▃▄▄▄▄▄▄▄▄▄▄▄████████████████████</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▂▃▄▅▅▆▆▇▇▇▇▇██████████████████████████</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▂▂▂▂▃▄▄▅▅▆▆▆▇▇▇▇███████▇▇████████████</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▂▂▂▃▄▄▅▅▆▆▆▆▇▇█████▇█▇▇████████████</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇█▇█▇▇▇▇██████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0012</td></tr><tr><td>details/n_training_tokens</td><td>199966720</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>24.6582</td></tr><tr><td>losses/mse_loss</td><td>0.13063</td></tr><tr><td>losses/overall_loss</td><td>0.22926</td></tr><tr><td>metrics/CE_loss_score</td><td>0.87109</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>10.375</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>4.59375</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>3.76562</td></tr><tr><td>metrics/explained_variance</td><td>0.78125</td></tr><tr><td>metrics/explained_variance_std</td><td>0.09521</td></tr><tr><td>metrics/l0</td><td>24.18628</td></tr><tr><td>metrics/l2_norm</td><td>21.125</td></tr><tr><td>metrics/l2_ratio</td><td>0.92969</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-4.88423</td></tr><tr><td>sparsity/below_1e-5</td><td>4101</td></tr><tr><td>sparsity/below_1e-6</td><td>1918</td></tr><tr><td>sparsity/dead_features</td><td>795</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>254.25952</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">8192-L1-0.004-LR-0.0012-Tokens-1.000e+08</strong> at: <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/yet79nwa' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/yet79nwa</a><br/>Synced 7 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240225_014425-yet79nwa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 8192-L1-0.004-LR-0.0012-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 4.194304\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 2097.152\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 48 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e8104a243047f18d0f756b69f1a7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is tokenized! Updating config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48828| MSE Loss 0.132 | L1 0.099: : 200003584it [53:00, 74117.92it/s]                             /home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (yet79nwa) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/mats_sae_training/scripts/wandb/run-20240225_023737-jnjexs04</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/jnjexs04' target=\"_blank\">8192-L1-0.004-LR-0.0012-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/jnjexs04' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/jnjexs04</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 13.375\n",
      "New distances: 10.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (yet79nwa) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "48828| MSE Loss 0.132 | L1 0.099: : 200003584it [53:17, 62542.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to checkpoints/zto7j8v6/final_sparse_autoencoder_gelu-2l_blocks.0.hook_mlp_out_8192.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5709783e7ff4da9b3fab85445e2dfbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='16.118 MB of 16.118 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>▃█▆▅▃▂▆▆▄▃▅▁▄▄▁▅▂▄▇▅▃▅▃▃▄▃▄▃▂▄▄▃▅▃▃▄▃▃▃▄</td></tr><tr><td>losses/mse_loss</td><td>█▆▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▅▆▇▆▇▇▇▇▇▇▇█▇█▇██████▇██▇██▇███▇█▇█████</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▂▅▇▅▂▃▅▅▅▃▁▃▂█▆▂▅▇▃▇▃▇▃▆▇▅▃▅▃▅▅▁▆▂▂▅▅▅▂▆</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>█▅▃▃▃▃▂▃▂▂▂▂▂▃▃▂▂▂▂▂▂▃▂▃▂▂▂▁▁▁▂▂▃▂▁▂▂▃▂▂</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▄▅▃▅▅▅▄▇▂▁▃▄▄▄▆▄▄▅▅▃▅█▂▇▄▄▃▂▂▁▃▃▅▄▂▄▄▇▅▅</td></tr><tr><td>metrics/explained_variance</td><td>▁▃▅▆▆▆▇▇▇▇▇▇▇▇▇██▇██████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▆▅▄▄▄▃▃▃▃▂▂▁▃▃▁▂▂▁▂▁▁▂▁▁▁▁▂▂▁▂▁▂▁▁▂▂▂▂▁</td></tr><tr><td>metrics/l0</td><td>█▄▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂</td></tr><tr><td>metrics/l2_norm</td><td>▁▃▄▅▆▅▅▇▆▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇██▇▇▇█▇▇▇█▇█▇█</td></tr><tr><td>metrics/l2_ratio</td><td>▁▃▄▅▆▅▅▇▆▆▇▇▇▇▇▇▇██▇█▇▇▇▇▇████▇█▇███▇███</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>█▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁▁▂▃▄▅▅▆▆▆▇▇▇▇██████████████████████████</td></tr><tr><td>sparsity/below_1e-6</td><td>▁▁▁▁▂▂▂▃▄▄▅▅▆▆▆▇▇▇██████▇█▇▇█▇██▇█████▇█</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▂▂▃▃▄▄▄▅▆▆▆▆▆▇▇█▇██▇█▇▇▇▇██▇████▇██</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▂▂▂▂▃▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇▇█▇▇▇▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0012</td></tr><tr><td>details/n_training_tokens</td><td>199966720</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>24.41406</td></tr><tr><td>losses/mse_loss</td><td>0.15241</td></tr><tr><td>losses/overall_loss</td><td>0.25006</td></tr><tr><td>metrics/CE_loss_score</td><td>0.86719</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>10.375</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>4.625</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>3.76562</td></tr><tr><td>metrics/explained_variance</td><td>0.74609</td></tr><tr><td>metrics/explained_variance_std</td><td>0.11084</td></tr><tr><td>metrics/l0</td><td>24.25073</td></tr><tr><td>metrics/l2_norm</td><td>18.875</td></tr><tr><td>metrics/l2_ratio</td><td>0.83203</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-4.84105</td></tr><tr><td>sparsity/below_1e-5</td><td>4105</td></tr><tr><td>sparsity/below_1e-6</td><td>1812</td></tr><tr><td>sparsity/dead_features</td><td>802</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>258.2323</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">8192-L1-0.004-LR-0.0012-Tokens-1.000e+08</strong> at: <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/jnjexs04' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/jnjexs04</a><br/>Synced 7 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240225_023737-jnjexs04/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 16384-L1-0.004-LR-0.0012-Tokens-1.000e+08\n",
      "n_tokens_per_buffer (millions): 4.194304\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.004096\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 2097.152\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 48 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-2l into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fb318afadd4c97bcfb8e97f6c96fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is tokenized! Updating config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (jnjexs04) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),/home/paperspace/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:2171: UserWarning: Run (jnjexs04) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "48828| MSE Loss 0.153 | L1 0.098: : 200003584it [54:38, 61002.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/mats_sae_training/scripts/wandb/run-20240225_033229-c2fnqpzd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/c2fnqpzd' target=\"_blank\">16384-L1-0.004-LR-0.0012-Tokens-1.000e+08</a></strong> to <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/c2fnqpzd' target=\"_blank\">https://wandb.ai/jbloom/mats_sae_training_language_models_gelu_2l_finetuning_experiment/runs/c2fnqpzd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 13.375\n",
      "New distances: 10.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26098| MSE Loss 0.138 | L1 0.097:  53%|█████▎    | 106897408/200000000 [28:22<08:39, 179353.78it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 56\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m finetuning_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munrotated_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m LanguageModelSAERunnerConfig(\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# Data Generating Function (Model + Training Distibuion)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     53\u001b[0m         )\n\u001b[0;32m---> 56\u001b[0m     sparse_autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mlanguage_model_sae_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mats_sae_training/scripts/../sae_training/lm_runner.py:32\u001b[0m, in \u001b[0;36mlanguage_model_sae_runner\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     29\u001b[0m     wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mwandb_project, config\u001b[38;5;241m=\u001b[39mcfg, name\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mrun_name)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# train SAE\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m sparse_autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sae_on_language_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_autoencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivations_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_checkpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_sampling_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_sampling_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdead_feature_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdead_feature_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_to_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_log_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwandb_log_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mlog_to_wandb:\n\u001b[1;32m     45\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/mats_sae_training/scripts/../sae_training/train_sae_on_language_model.py:174\u001b[0m, in \u001b[0;36mtrain_sae_on_language_model\u001b[0;34m(model, sparse_autoencoder, activation_store, batch_size, n_checkpoints, feature_sampling_window, dead_feature_threshold, use_wandb, wandb_log_frequency)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_wandb \u001b[38;5;129;01mand\u001b[39;00m ((n_training_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m (wandb_log_frequency \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    173\u001b[0m     sparse_autoencoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 174\u001b[0m     \u001b[43mrun_evals\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_autoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_training_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     sparse_autoencoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    177\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_training_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m| MSE Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | L1 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml1_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mats_sae_training/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mats_sae_training/scripts/../sae_training/evals.py:107\u001b[0m, in \u001b[0;36mrun_evals\u001b[0;34m(sparse_autoencoder, activation_store, model, n_training_steps)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks\u001b[38;5;241m=\u001b[39m[(hook_point, partial(replacement_hook))]):\n\u001b[1;32m     99\u001b[0m     _, new_cache \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_with_cache(\n\u001b[1;32m    100\u001b[0m         eval_tokens, names_filter\u001b[38;5;241m=\u001b[39m[get_act_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m, hook_point_layer)]\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m     patterns_reconstructed \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    103\u001b[0m         \u001b[43mnew_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mget_act_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpattern\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_point_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_point_head_index\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 107\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     )\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m new_cache\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# get attn when using reconstructed activations\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26098| MSE Loss 0.138 | L1 0.097:  53%|█████▎    | 106901504/200000000 [28:34<08:39, 179353.78it/s]wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for expansion_factor in [8]:\n",
    "    for finetuning_method in [\"decoder\"]:\n",
    "        cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "            # Data Generating Function (Model + Training Distibuion)\n",
    "            model_name = \"gelu-2l\",\n",
    "            hook_point = \"blocks.0.hook_mlp_out\",\n",
    "            hook_point_layer = 0,\n",
    "            d_in = 512,\n",
    "            dataset_path = \"NeelNanda/c4-tokenized-2b\",\n",
    "            is_dataset_tokenized=True,\n",
    "            \n",
    "            # SAE Parameters\n",
    "            expansion_factor = expansion_factor,\n",
    "            b_dec_init_method=\"mean\", # geometric median is better but slower to get started\n",
    "            normalize_activations = True,\n",
    "            use_pre_encoder_bias= False,\n",
    "            \n",
    "            # Training Parameters\n",
    "            lr = 0.0012,\n",
    "            lr_scheduler_name=\"constantwithwarmup\",\n",
    "            l1_coefficient = 0.004,\n",
    "            train_batch_size = 4096,\n",
    "            context_size = 1024,\n",
    "            adam_beta1 = 0,\n",
    "            adam_beta2 = 0.9999,\n",
    "            finetuning_method = finetuning_method,\n",
    "            \n",
    "            # Activation Store Parameters\n",
    "            n_batches_in_buffer = 128,\n",
    "            total_training_tokens = 1_000_000 * 100, \n",
    "            fine_tune_tokens = 1_000_000 * 100,\n",
    "            store_batch_size = 32,\n",
    "            \n",
    "            # Resampling protocol\n",
    "            mse_loss_normalization = None,\n",
    "            ghost_grads=None,#\"residual\",\n",
    "            feature_sampling_window = 500,\n",
    "            dead_feature_window=500,\n",
    "            dead_feature_threshold = 1e-5,\n",
    "            \n",
    "            # WANDB\n",
    "            log_to_wandb = True,\n",
    "            wandb_project= \"mats_sae_training_language_models_gelu_2l_finetuning_experiment\",\n",
    "            wandb_log_frequency=10,\n",
    "            \n",
    "            # Misc\n",
    "            device = device,\n",
    "            seed = 42,\n",
    "            n_checkpoints = 0,\n",
    "            checkpoint_path = \"checkpoints\",\n",
    "            dtype = torch.bfloat16,\n",
    "            )\n",
    "\n",
    "\n",
    "        sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 - Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "layer = 3\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = f\"blocks.{layer}.hook_resid_pre\",\n",
    "    hook_point_layer = layer,\n",
    "    d_in = 768,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 32, # determines the dimension of the SAE.\n",
    "    b_dec_init_method = \"mean\", # geometric median is better but slower to get started\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 0.0004,\n",
    "    l1_coefficient = 0.00008,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    lr_warm_up_steps=5000,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 300, # 200M tokens seems doable overnight.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=True,\n",
    "    feature_sampling_window = 2500,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold = 1e-8,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_models_resid_pre_test\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=100,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"cuda\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 10,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythia 70-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys \n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "import cProfile\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"pythia-70m-deduped\",\n",
    "    hook_point = \"blocks.0.hook_mlp_out\",\n",
    "    hook_point_layer = 0,\n",
    "    d_in = 512,\n",
    "    dataset_path = \"EleutherAI/the_pile_deduplicated\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 3e-4,\n",
    "    l1_coefficient = 4e-5,\n",
    "    train_batch_size = 8192,\n",
    "    context_size = 128,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    lr_warm_up_steps=10_000,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 64,\n",
    "    total_training_tokens = 1_000_000 * 800, \n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'anthropic',\n",
    "    feature_sampling_window = 2000, # Doesn't currently matter.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=40000,\n",
    "    dead_feature_threshold = 1e-8,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_benchmark_tests\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=20,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"cuda\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythia 70M Hook Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"pythia-70m-deduped\",\n",
    "    hook_point = \"blocks.2.attn.hook_q\",\n",
    "    hook_point_layer = 2,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"EleutherAI/the_pile_deduplicated\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 16,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 0.0012,\n",
    "    l1_coefficient = 0.003,\n",
    "    lr_scheduler_name=\"constantwithwarmup\",\n",
    "    lr_warm_up_steps=1000, # about 4 million tokens.\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 1500,\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'anthropic',\n",
    "    feature_sampling_window = 1000,# doesn't do anything currently.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    resample_batches=8,\n",
    "    dead_feature_window=60000,\n",
    "    dead_feature_threshold = 1e-5,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_pythia_70M_hook_q_L2H7\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=100,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 15,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"tiny-stories-2L-33M\",\n",
    "    hook_point = \"blocks.1.mlp.hook_post\",\n",
    "    hook_point_layer = 1,\n",
    "    d_in = 4096,\n",
    "    dataset_path = \"roneneldan/TinyStories\",\n",
    "    is_dataset_tokenized=False,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 4,\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-4,\n",
    "    l1_coefficient = 3e-4,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 128,\n",
    "    total_training_tokens = 1_000_000 * 10, # want 500M eventually.\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 2500, # Doesn't currently matter.\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=1250,\n",
    "    dead_feature_threshold = 0.0005,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_language_benchmark_tests\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=10,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sae_training.toy_model_runner import SAEToyModelRunnerConfig, toy_model_sae_runner\n",
    "\n",
    "\n",
    "cfg = SAEToyModelRunnerConfig(\n",
    "    \n",
    "    # Model Details\n",
    "    n_features=200,\n",
    "    n_hidden=5,\n",
    "    n_correlated_pairs=0,\n",
    "    n_anticorrelated_pairs=0,\n",
    "    feature_probability=0.025,\n",
    "    model_training_steps=10_000,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    d_sae=240,\n",
    "    l1_coefficient=0.001,\n",
    "    \n",
    "    # SAE Train Config\n",
    "    train_batch_size=1028,\n",
    "    feature_sampling_window=3_000,\n",
    "    dead_feature_window=1_000,\n",
    "    feature_reinit_scale=0.5,\n",
    "    total_training_tokens=4096*300,\n",
    "    \n",
    "    # Other parameters\n",
    "    log_to_wandb=True,\n",
    "    wandb_project=\"sae-training-test\",\n",
    "    wandb_log_frequency=5,\n",
    "    device=\"mps\",\n",
    ")\n",
    "\n",
    "trained_sae = toy_model_sae_runner(cfg)\n",
    "\n",
    "assert trained_sae is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run caching of activations to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "\n",
    "from sae_training.config import CacheActivationsRunnerConfig\n",
    "from sae_training.cache_activations_runner import cache_activations_runner\n",
    "\n",
    "cfg = CacheActivationsRunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = f\"blocks.{layer}.hook_resid_pre\",\n",
    "    hook_point_layer = 10,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 16,\n",
    "    total_training_tokens = 500_000_000, \n",
    "    store_batch_size = 32,\n",
    "\n",
    "    # Activation caching shuffle parameters\n",
    "    n_shuffles_final = 16,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "cache_activations_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an SAE using the cached activations stored on disk\n",
    "Pass `use_cached_activations=True` into the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.hook_resid_pre\",\n",
    "    hook_point_layer = 11,\n",
    "    d_in = 768,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64, # determines the dimension of the SAE.\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-5,\n",
    "    l1_coefficient = 5e-4,\n",
    "    lr_scheduler_name=None,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 64,\n",
    "    total_training_tokens = 200_000,\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=5000,\n",
    "    dead_feature_threshold = 1e-7,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=50,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 5,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.lm_runner import language_model_sae_runner\n",
    "\n",
    "\n",
    "\n",
    "# for l1_coefficient in [9e-4,8e-4,7e-4]:\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name = \"gpt2-small\",\n",
    "    hook_point = \"blocks.10.attn.hook_q\",\n",
    "    hook_point_layer = 10,\n",
    "    hook_point_head_index=7,\n",
    "    d_in = 64,\n",
    "    dataset_path = \"Skylion007/openwebtext\",\n",
    "    is_dataset_tokenized=False,\n",
    "    use_cached_activations=True,\n",
    "    cached_activations_path=\"../activations/\",\n",
    "    \n",
    "    # SAE Parameters\n",
    "    expansion_factor = 64, # determines the dimension of the SAE. (64*64 = 4096, 64*4*64 = 32768)\n",
    "    \n",
    "    # Training Parameters\n",
    "    lr = 1e-3,\n",
    "    l1_coefficient = 2e-4,\n",
    "    # lr_scheduler_name=\"LinearWarmupDecay\",\n",
    "    lr_warm_up_steps=2200,\n",
    "    train_batch_size = 4096,\n",
    "    context_size = 128,\n",
    "    \n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer = 512,\n",
    "    total_training_tokens = 3_000_000,\n",
    "    store_batch_size = 32,\n",
    "    \n",
    "    # Resampling protocol\n",
    "    feature_sampling_method = 'l2',\n",
    "    feature_sampling_window = 1000,\n",
    "    feature_reinit_scale = 0.2,\n",
    "    dead_feature_window=200,\n",
    "    dead_feature_threshold = 5e-6,\n",
    "    \n",
    "    # WANDB\n",
    "    log_to_wandb = True,\n",
    "    wandb_project= \"mats_sae_training_gpt2_small_hook_q_dev\",\n",
    "    wandb_entity = None,\n",
    "    wandb_log_frequency=5,\n",
    "    \n",
    "    # Misc\n",
    "    device = \"mps\",\n",
    "    seed = 42,\n",
    "    n_checkpoints = 0,\n",
    "    checkpoint_path = \"checkpoints\",\n",
    "    dtype = torch.float32,\n",
    "    )\n",
    "\n",
    "# cfg.d_sae\n",
    "sparse_autoencoder = language_model_sae_runner(cfg)\n",
    "# assert sparse_autoencoder is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
