{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building run_with_saes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import webbrowser\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "path_to_html = \"../week_8_jan/gpt2_small_features_layer_5\"\n",
    "def render_feature_dashboard(feature_id):\n",
    "    \n",
    "    path = f\"{path_to_html}/data_{feature_id:04}.html\"\n",
    "    \n",
    "    print(f\"Feature {feature_id}\")\n",
    "    if os.path.exists(path):\n",
    "        # with open(path, \"r\") as f:\n",
    "        #     html = f.read()\n",
    "        #     display(HTML(html))\n",
    "        webbrowser.open_new_tab(\"file://\" + os.path.abspath(path))\n",
    "    else:\n",
    "        print(\"No HTML file found\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_all_layer_saes = \"../GPT2-small-SAEs/\" \n",
    "\n",
    "log_sparsity_files = os.listdir(path_to_all_layer_saes)\n",
    "# print(log_sparsity_files)\n",
    "model_files = [f for f in log_sparsity_files if \"log\" not in f]\n",
    "model_files = sorted(model_files, key=lambda x: int(x.split(\".\")[1]))\n",
    "display(model_files)\n",
    "\n",
    "log_sparsity_files = [f for f in log_sparsity_files if \"log_feature_sparsity\" in f]\n",
    "log_sparsity_files = sorted(log_sparsity_files, key=lambda x: int(x.split(\".\")[1]))\n",
    "log_sparsity_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "\n",
    "gpt2_small_sparse_autoencoders = {}\n",
    "for path in model_files:\n",
    "    layer = int(path.split(\".\")[1])\n",
    "    print(f\"Loading layer {layer}\")\n",
    "    sae = SparseAutoencoder.load_from_pretrained(f\"{path_to_all_layer_saes}/{path}\")\n",
    "    sae.cfg.use_ghost_grads = False\n",
    "    gpt2_small_sparse_autoencoders[sae.cfg.hook_point] = sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "model, sparse_autoencoder, activation_store = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path = \"../GPT2-small-SAEs/final_sparse_autoencoder_gpt2-small_blocks.7.hook_resid_pre_24576.pt\"\n",
    ")\n",
    "sparse_autoencoder.cfg.use_ghost_grads = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = sparse_autoencoder.cfg\n",
    "\n",
    "from sae_training.activations_store import ActivationStore\n",
    "\n",
    "\n",
    "activation_store = ActivationStore(\n",
    "    cfg,\n",
    "    model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def get_recons_loss(sparse_autoencoder, model, batch_tokens, hook_point):\n",
    "    loss = model(batch_tokens, return_type=\"loss\", loss_per_token=True)\n",
    "\n",
    "    head_index = sparse_autoencoder.cfg.hook_point_head_index\n",
    "\n",
    "    def mean_ablate_hook(mlp_post, hook):\n",
    "        mlp_post[:] = mlp_post.mean([0, 1]).to(mlp_post.dtype)\n",
    "        return mlp_post\n",
    "\n",
    "    def zero_ablate_hook(mlp_post, hook):\n",
    "        mlp_post[:] = 0.0\n",
    "        return mlp_post\n",
    "\n",
    "    def no_replacement_hook(activations, hook):\n",
    "        return activations\n",
    "\n",
    "    def standard_replacement_hook(activations, hook):\n",
    "        activations = sparse_autoencoder.forward(activations)[0].to(activations.dtype)\n",
    "        return activations\n",
    "\n",
    "    def head_replacement_hook(activations, hook):\n",
    "        new_actions = sparse_autoencoder.forward(activations[:,:,head_index])[0].to(activations.dtype)\n",
    "        activations[:,:,head_index] = new_actions\n",
    "        return activations\n",
    "\n",
    "    replacement_hook = standard_replacement_hook if head_index is None else head_replacement_hook\n",
    "\n",
    "    no_replacement_loss = model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(hook_point, partial(no_replacement_hook))],\n",
    "        loss_per_token=True,\n",
    "    )\n",
    "    \n",
    "    recons_loss = model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(hook_point, partial(replacement_hook))],\n",
    "        loss_per_token=True,\n",
    "    )\n",
    "\n",
    "    zero_abl_loss = model.run_with_hooks(\n",
    "        batch_tokens, return_type=\"loss\", fwd_hooks=[(hook_point, zero_ablate_hook)],\n",
    "        loss_per_token=True,\n",
    "    )\n",
    "\n",
    "    score = (zero_abl_loss - recons_loss) / (zero_abl_loss - loss)\n",
    "\n",
    "    return score, loss, recons_loss, zero_abl_loss, no_replacement_loss\n",
    "\n",
    "batch_tokens = activation_store.get_batch_tokens()\n",
    "score, loss, recons_loss, zero_abl_loss, no_replacement_loss = get_recons_loss(sparse_autoencoder, model, batch_tokens, activation_store.cfg.hook_point)\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recons_loss_batched(sparse_autoencoder, model, activation_store, n_batches = 100):\n",
    "    \n",
    "    losses = []\n",
    "    for _ in tqdm(range(n_batches)):\n",
    "        batch_tokens = activation_store.get_batch_tokens()\n",
    "        score, loss, recons_loss, zero_abl_loss, no_replacement_loss = get_recons_loss(sparse_autoencoder, model, batch_tokens, activation_store.cfg.hook_point)\n",
    "        losses.append((score.mean().item(), loss.mean().item(), recons_loss.mean().item(), zero_abl_loss.mean().item(), no_replacement_loss.mean().item()))\n",
    "\n",
    "    losses = pd.DataFrame(losses, columns=[\"score\", \"loss\", \"recons_loss\", \"zero_abl_loss\", \"no_replacement_loss\"])\n",
    "    \n",
    "    return losses\n",
    "\n",
    "ce_losses = recons_loss_batched(sparse_autoencoder, model, activation_store, n_batches  = 10)\n",
    "ce_losses.recons_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_losses.loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variance_explained(model, sparse_autoencoder, batch_tokens):\n",
    "    logits, cache = model.run_with_cache(batch_tokens, return_type=\"loss\")\n",
    "    x = cache[activation_store.cfg.hook_point]\n",
    "    (\n",
    "        sae_out,\n",
    "        feature_acts,\n",
    "        loss,\n",
    "        mse_loss,\n",
    "        l1_loss,\n",
    "        mse_loss_ghost_resid,\n",
    "    ) = sparse_autoencoder(x)\n",
    "    \n",
    "    x = x.float().cpu()[:,1:,:]\n",
    "    x_centred = x - x.mean(-1, keepdim=True)\n",
    "    sae_out = sae_out[:,1:,:].cpu()\n",
    "\n",
    "\n",
    "    # MSE Loss\n",
    "    mse_loss_sam = (sae_out - x).pow(2) / x_centred.pow(2).sum(dim=-1, keepdim=True).sqrt()\n",
    "    mse_loss = (sae_out - x).pow(2) / (x.pow(2)).sum(dim=-1, keepdim=True).sqrt()\n",
    "\n",
    "    # Variance Explained\n",
    "    per_token_l2_dist = (sae_out - x).pow(2).sum(dim=-1).squeeze()\n",
    "    total_variance = x.pow(2).sum(dim=-1, keepdim=True).squeeze()\n",
    "    total_variance_with_centering = x_centred.pow(2).sum(dim=-1, keepdim=True).squeeze()\n",
    "\n",
    "    variance_explained = 1 - (per_token_l2_dist / total_variance)\n",
    "    variance_explained_sams = 1 - (per_token_l2_dist / total_variance_with_centering)\n",
    "    \n",
    "\n",
    "    return mse_loss_sam, mse_loss, variance_explained, variance_explained_sams\n",
    "\n",
    "mse_loss_sam, mse_loss, variance_explained, variance_explained_sams = get_variance_explained(model, sparse_autoencoder, batch_tokens)\n",
    "\n",
    "px.line(mse_loss.mean(-1).T.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_variance_explained_batched(\n",
    "    model, sparse_autoencoder, activation_store, n_batches=100\n",
    "):\n",
    "    sams_losses = []\n",
    "    our_losses = []\n",
    "    total_variances_1 = []\n",
    "    total_variances_2 = []\n",
    "\n",
    "    for _ in tqdm(range(n_batches)):\n",
    "        batch_tokens = activation_store.get_batch_tokens()\n",
    "        (\n",
    "            sams_loss,\n",
    "            our_loss,\n",
    "            total_variance_1,\n",
    "            total_variance_2,\n",
    "        ) = get_variance_explained(model, sparse_autoencoder, batch_tokens)\n",
    "        sams_losses.append(sams_loss.mean().item())\n",
    "        our_losses.append(our_loss.mean().item())\n",
    "        total_variances_1.append(total_variance_1.mean().item())\n",
    "        total_variances_2.append(total_variance_2.mean().item())\n",
    "\n",
    "    losses = pd.DataFrame(\n",
    "        {\n",
    "            \"sams\": sams_losses,\n",
    "            \"our\": our_losses,\n",
    "            \"variance_explained_ours\": total_variances_1,\n",
    "            \"variance_explained_sams\": total_variances_2,\n",
    "        }\n",
    "    )\n",
    "    return losses\n",
    "\n",
    "\n",
    "batch_tokens = activation_store.get_batch_tokens()\n",
    "losses = get_variance_explained_batched(\n",
    "    model, sparse_autoencoder, activation_store, n_batches=10\n",
    ")\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses.variance_explained_ours.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses.variance_explained_sams.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_point = activation_store.cfg.hook_point\n",
    "prompt = \"The quick brown fox jumps over the lazy\"\n",
    "answer = \" dog\"\n",
    "prompt = \" John and Mary went to the park. Then John said to\"\n",
    "answer = \" Mary\"\n",
    "utils.test_prompt(prompt, answer, model)\n",
    "\n",
    "    \n",
    "logits, cache = model.run_with_cache(prompt)\n",
    "activations = cache[hook_point]\n",
    "def standard_replacement_hook(activations, hook: HookPoint):\n",
    "    activations = sparse_autoencoder.forward(activations)[0].to(activations.dtype)\n",
    "    return activations\n",
    "    \n",
    "print(model.generate(prompt, max_new_tokens=20, stop_at_eos=False, temperature=0))\n",
    "with model.hooks(fwd_hooks=[(hook_point, standard_replacement_hook)]):\n",
    "\n",
    "    utils.test_prompt(prompt = prompt, answer = answer, model = model)\n",
    "    print(model.generate(prompt, max_new_tokens=20, stop_at_eos=False, temperature=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Sam's SAE's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sams_layer1_sae_path = \"layer_1_resid_post_ae_245000.pt\"\n",
    "\n",
    "weights = torch.load(sams_layer1_sae_path, map_location=\"cpu\")\n",
    "display(weights.keys())\n",
    "\n",
    "# rename keys\n",
    "new_weights = {}\n",
    "rename_map = {\n",
    "    \"bias\":\"b_dec\",\n",
    "    \"encoder.bias\":\"b_enc\",\n",
    "    \"decoder.weight\":\"W_dec\",\n",
    "    \"encoder.weight\":\"W_enc\"\n",
    "}\n",
    "\n",
    "for k, v in rename_map.items():\n",
    "    new_weights[v] = weights[k]\n",
    "    \n",
    "# rotate the following weights\n",
    "weights_to_rotate = [\"W_enc\", \"W_dec\"]\n",
    "for w in weights_to_rotate:\n",
    "    new_weights[w] = new_weights[w].T\n",
    "    \n",
    "display(new_weights.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "model, sparse_autoencoder, activation_store = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path = \"../GPT2-small-SAEs/final_sparse_autoencoder_gpt2-small_blocks.2.hook_resid_pre_24576.pt\"\n",
    ")\n",
    "sparse_autoencoder.cfg.use_ghost_grads = False # reside pre 2 should be good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I want to make an SAE which I can load this SAE weight into. \n",
    "from sae_training.activations_store import ActivationsStore\n",
    "from sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    model_name = \"pythia-70m-deduped\",\n",
    "    hook_point = \"blocks.1.hook_resid_post\",\n",
    "    dataset_path= \"EleutherAI/the_pile_deduplicated\",\n",
    "    hook_point_layer = 1,\n",
    "    feature_sampling_method=None,\n",
    "    d_in = 512,\n",
    "    lr = 0.0,\n",
    "    l1_coefficient = 0.0,\n",
    "    expansion_factor = 64,\n",
    "    device=\"mps\",\n",
    "    store_batch_size = 32,\n",
    "    n_batches_in_buffer=128,\n",
    "    use_ghost_grads=False,\n",
    ")\n",
    "\n",
    "sams_sparse_autoencoder = SparseAutoencoder(cfg)\n",
    "print(sams_sparse_autoencoder.state_dict().keys())\n",
    "sams_sparse_autoencoder.load_state_dict(new_weights)\n",
    "sams_sparse_autoencoder.to(\"mps\")\n",
    "pythia_70m_model = HookedTransformer.from_pretrained(\"pythia-70m-deduped\", device=\"mps\", fold_ln=True)\n",
    "pythia_70m_activation_store = ActivationsStore(sams_sparse_autoencoder.cfg, pythia_70m_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tokens = pythia_70m_activation_store.get_batch_tokens()\n",
    "batch_tokens.shape\n",
    "loss = pythia_70m_model(batch_tokens, return_type=\"loss\")\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pythia 70m deduped\n",
    "activations = pythia_70m_activation_store.next_batch()\n",
    "sae_out, feature_acts, loss, mse_loss, l1_loss, mse_loss_ghost_resid = sams_sparse_autoencoder(activations)\n",
    "print(\"Norms\")\n",
    "print(\"In Norm\", sae_out.norm(dim=0).mean().item())\n",
    "print(\"Out norm\", activations.norm(dim=0).mean().item())\n",
    "print(\"-\"*20)\n",
    "print(\"Sparsity\")\n",
    "print(\"L1\", feature_acts.sum(dim=1).mean().item())\n",
    "print(\"L0\", (feature_acts > 0).float().sum(dim=1).mean().item()) # Way too many features firing.\n",
    "print(\"-\"*20)\n",
    "print(\"Loss\")\n",
    "print(\"MSE\", mse_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pythia 70m deduped\n",
    "activations = pythia_70m_activation_store.next_batch()\n",
    "activation_plus_b_dec = activations + sams_sparse_autoencoder.b_dec\n",
    "sae_out, feature_acts, loss, mse_loss, l1_loss, mse_loss_ghost_resid = sams_sparse_autoencoder(activation_plus_b_dec)\n",
    "print(\"Norms\")\n",
    "print(\"In Norm\", sae_out.norm(dim=0).mean().item())\n",
    "print(\"Out norm\", activations.norm(dim=0).mean().item())\n",
    "print(\"-\"*20)\n",
    "print(\"Sparsity\")\n",
    "print(\"L1\", feature_acts.sum(dim=1).mean().item())\n",
    "print(\"L0\", (feature_acts > 0).float().sum(dim=1).mean().item()) # Way too many features firing.\n",
    "print(\"-\"*20)\n",
    "print(\"Loss\")\n",
    "print(\"MSE\", mse_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is is a GPT2 small residual stream SAE (layer 2 resid pre) \n",
    "activations = activation_store.next_batch()\n",
    "sae_out, feature_acts, loss, mse_loss, l1_loss, mse_loss_ghost_resid = sparse_autoencoder(activations)\n",
    "print(\"Norms\")\n",
    "print(\"In Norm\", sae_out.norm(dim=0).mean().item())\n",
    "print(\"Out norm\", activations.norm(dim=0).mean().item())\n",
    "print(\"-\"*20)\n",
    "print(\"Sparsity\")\n",
    "print(\"L1\", feature_acts.sum(dim=1).mean().item())\n",
    "print(\"L0\", (feature_acts > 0).float().sum(dim=1).mean().item()) # Way too many features firing.\n",
    "print(\"-\"*20)\n",
    "print(\"Loss\")\n",
    "print(\"MSE\", mse_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def get_recons_loss(sparse_autoencoder, model, batch_tokens, hook_point, add_b_dec = False):\n",
    "    loss = model(batch_tokens, return_type=\"loss\", loss_per_token=True)\n",
    "\n",
    "    head_index = sparse_autoencoder.cfg.hook_point_head_index\n",
    "\n",
    "    def mean_ablate_hook(mlp_post, hook):\n",
    "        mlp_post[:] = mlp_post.mean([0, 1]).to(mlp_post.dtype)\n",
    "        return mlp_post\n",
    "\n",
    "    def zero_ablate_hook(mlp_post, hook):\n",
    "        mlp_post[:] = 0.0\n",
    "        return mlp_post\n",
    "\n",
    "    def no_replacement_hook(activations, hook):\n",
    "        return activations\n",
    "\n",
    "    def standard_replacement_hook(activations, hook):\n",
    "        if add_b_dec:\n",
    "            activations = activations + sparse_autoencoder.b_dec\n",
    "        activations = sparse_autoencoder.forward(activations)[0].to(activations.dtype)\n",
    "        return activations\n",
    "\n",
    "    def head_replacement_hook(activations, hook):\n",
    "        new_actions = sparse_autoencoder.forward(activations[:,:,head_index])[0].to(activations.dtype)\n",
    "        activations[:,:,head_index] = new_actions\n",
    "        return activations\n",
    "\n",
    "    replacement_hook = standard_replacement_hook if head_index is None else head_replacement_hook\n",
    "\n",
    "    no_replacement_loss = model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(hook_point, partial(no_replacement_hook))],\n",
    "        loss_per_token=True,\n",
    "    )\n",
    "    \n",
    "    recons_loss = model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(hook_point, partial(replacement_hook))],\n",
    "        loss_per_token=True,\n",
    "    )\n",
    "\n",
    "    zero_abl_loss = model.run_with_hooks(\n",
    "        batch_tokens, return_type=\"loss\", fwd_hooks=[(hook_point, zero_ablate_hook)],\n",
    "        loss_per_token=True,\n",
    "    )\n",
    "\n",
    "    score = (zero_abl_loss - recons_loss) / (zero_abl_loss - loss)\n",
    "\n",
    "    return score, loss, recons_loss, zero_abl_loss, no_replacement_loss\n",
    "\n",
    "def recons_loss_batched(sparse_autoencoder, model, activation_store, n_batches = 100, add_b_dec = False):\n",
    "    \n",
    "    losses = []\n",
    "    for _ in tqdm(range(n_batches)):\n",
    "        batch_tokens = activation_store.get_batch_tokens()\n",
    "        score, loss, recons_loss, zero_abl_loss, no_replacement_loss = get_recons_loss(\n",
    "            sparse_autoencoder, model, batch_tokens, activation_store.cfg.hook_point,\n",
    "            add_b_dec = add_b_dec)\n",
    "        losses.append((score.mean().item(), loss.mean().item(), recons_loss.mean().item(), zero_abl_loss.mean().item(), no_replacement_loss.mean().item()))\n",
    "\n",
    "    losses = pd.DataFrame(losses, columns=[\"score\", \"loss\", \"recons_loss\", \"zero_abl_loss\", \"no_replacement_loss\"])\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "ce_losses = recons_loss_batched(sams_sparse_autoencoder, pythia_70m_model, pythia_70m_activation_store, n_batches  = 100)\n",
    "display(ce_losses)\n",
    "print(ce_losses.score.mean())\n",
    "print(ce_losses.recons_loss.mean())\n",
    "ce_losses = recons_loss_batched(sams_sparse_autoencoder, pythia_70m_model, pythia_70m_activation_store, n_batches  = 100, add_b_dec=True)\n",
    "display(ce_losses)\n",
    "print(ce_losses.score.mean())\n",
    "print(ce_losses.recons_loss.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_losses = recons_loss_batched(sparse_autoencoder, model, activation_store, n_batches  = 100, add_b_dec=False)\n",
    "display(ce_losses)\n",
    "print(ce_losses.score.mean())\n",
    "print(ce_losses.recons_loss.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok let's just compare weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(sparse_autoencoder.b_enc.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
