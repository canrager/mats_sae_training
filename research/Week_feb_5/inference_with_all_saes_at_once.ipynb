{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building run_with_saes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "\n",
    "import joseph\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "\n",
    "reload(joseph.analysis)\n",
    "reload(joseph.visualisation)\n",
    "reload(joseph.utils)\n",
    "reload(joseph.data)\n",
    "\n",
    "from joseph.analysis import *\n",
    "from joseph.visualisation import *\n",
    "from joseph.utils import *\n",
    "from joseph.data import *\n",
    "\n",
    "# turn torch grad tracking off\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import webbrowser\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "path_to_html = \"../week_8_jan/gpt2_small_features_layer_5\"\n",
    "def render_feature_dashboard(feature_id):\n",
    "    \n",
    "    path = f\"{path_to_html}/data_{feature_id:04}.html\"\n",
    "    \n",
    "    print(f\"Feature {feature_id}\")\n",
    "    if os.path.exists(path):\n",
    "        # with open(path, \"r\") as f:\n",
    "        #     html = f.read()\n",
    "        #     display(HTML(html))\n",
    "        webbrowser.open_new_tab(\"file://\" + os.path.abspath(path))\n",
    "    else:\n",
    "        print(\"No HTML file found\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_all_layer_saes = \"../GPT2-small-SAEs/\" \n",
    "\n",
    "log_sparsity_files = os.listdir(path_to_all_layer_saes)\n",
    "# print(log_sparsity_files)\n",
    "model_files = [f for f in log_sparsity_files if \"log\" not in f]\n",
    "model_files = sorted(model_files, key=lambda x: int(x.split(\".\")[1]))\n",
    "display(model_files)\n",
    "\n",
    "log_sparsity_files = [f for f in log_sparsity_files if \"log_feature_sparsity\" in f]\n",
    "log_sparsity_files = sorted(log_sparsity_files, key=lambda x: int(x.split(\".\")[1]))\n",
    "log_sparsity_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "\n",
    "gpt2_small_sparse_autoencoders = {}\n",
    "for path in model_files:\n",
    "    layer = int(path.split(\".\")[1])\n",
    "    print(f\"Loading layer {layer}\")\n",
    "    sae = SparseAutoencoder.load_from_pretrained(f\"{path_to_all_layer_saes}/{path}\")\n",
    "    sae.cfg.use_ghost_grads = False\n",
    "    gpt2_small_sparse_autoencoders[sae.cfg.hook_point] = sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bracket_prompt():\n",
    "    \n",
    "    left_bracket_token = torch.tensor(model.to_single_token(\"(\")).unsqueeze(0).repeat(1,256)\n",
    "    right_bracket_token = torch.tensor(model.to_single_token(\")\")).unsqueeze(0).repeat(1,256)\n",
    "    random_tokens = torch.randint(0, 50257, (10,256))\n",
    "    \n",
    "    # add the brackets after the first two tokens and before the last two tokensx\n",
    "    prompt = torch.concat([random_tokens[:2], left_bracket_token, random_tokens[2:-2:], right_bracket_token, random_tokens[-2:]], dim=0).T\n",
    "    return prompt\n",
    "\n",
    "tokens = generate_bracket_prompt()\n",
    "(original_logits, original_loss), original_cache = model.run_with_cache(tokens, return_type=\"both\", loss_per_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts_cache = {}\n",
    "sae_out_cache = {}\n",
    "mse_loss_cache = {}\n",
    "\n",
    "for hook_point, sae in gpt2_small_sparse_autoencoders.items():\n",
    "    sae_out, feature_acts, _, mse_loss, _, _ = gpt2_small_sparse_autoencoders[hook_point](original_cache[hook_point])\n",
    "    feature_acts_cache[hook_point] = feature_acts\n",
    "    sae_out_cache[hook_point] = sae_out\n",
    "    mse_loss_cache[hook_point] = mse_loss.detach().item()\n",
    "    \n",
    "# get each feature acts and stacks them\n",
    "feature_acts_stacked = torch.stack([feature_acts for feature_acts in feature_acts_cache.values()], dim=0)\n",
    "feature_acts_stacked.shape # [n_saes, batch_size, n_tokens, n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go \n",
    "from plotly.subplots import make_subplots\n",
    "# create a 4 * 3 grid of subplots\n",
    "fig = make_subplots(rows=3, cols=4, subplot_titles=[f\"Layer {i}\" for i in range(1,13)])\n",
    "\n",
    "top_k = 5\n",
    "# add a line chart to each subplot\n",
    "for layer in range(12):\n",
    "    score_features_by = feature_acts_stacked[layer,:].mean(0)[2:9].sum(0)\n",
    "    vals, inds = torch.topk(score_features_by, top_k)\n",
    "    tmp_df = pd.DataFrame(feature_acts_stacked.mean(1)[layer,:,inds].cpu(),\n",
    "                          columns = [f\"Feature_{i}\" for i in inds],\n",
    "                          index = [f\"tok_{i}\" for i in range(12)])\n",
    "    # rename tok_2 to open bracket\n",
    "    tmp_df = tmp_df.rename(index={\"tok_2\": \"open_bracket\"})\n",
    "    # rename tok_9 to close bracket\n",
    "    tmp_df = tmp_df.rename(index={\"tok_9\": \"close_bracket\"})\n",
    "    for feature in tmp_df.columns:\n",
    "        fig.add_trace(go.Scatter(x=tmp_df.index, \n",
    "                                 y=tmp_df[feature],\n",
    "                                #  y=np.log10(tmp_df[feature]+ 1e-4), \n",
    "                                 mode=\"lines\",\n",
    "                                 name=feature,\n",
    "                                 hovertemplate=\"Token: %{x}<br>Activation: %{y}\" + f\"<br>Layer: {layer}\",\n",
    "                                 ), \n",
    "                      row=(layer//4)+1, col=(layer%4)+1)\n",
    "        # rotate x ticks\n",
    "        fig.update_xaxes(tickangle=45)\n",
    "    \n",
    "fig.update_layout(height=900, width=1600, title_text=f\"Top {top_k} Features in each layer (averaged in between parentheses context)\")\n",
    "# remove legend\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
